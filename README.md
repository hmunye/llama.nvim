<div align="center">
  
<img src="https://github.com/user-attachments/assets/d5d38206-7c72-48d4-9ebb-0ce5ac82c7b4#gh-light-mode-only" width="400px" alt="llama.nvim logo"/>
<img src="https://github.com/user-attachments/assets/77c9a2b8-923a-4160-acd2-95e934f270e8#gh-dark-mode-only"  width="400px" alt="llama.nvim logo"/>

##### AI in your terminal, powered locally

[![Neovim](https://img.shields.io/static/v1?&style=for-the-badge&label=Neovim&message=v0.10%2b&logo=neovim)](https://neovim.io)
[![Lua](https://img.shields.io/badge/Lua-blue.svg?style=for-the-badge&logo=lua)](http://www.lua.org)
</div>

## TOC
* [Overview](#overview)
* [Features](#features)

## Overview
**llama.nvim** is a Neovim plugin that provides local AI in your terminal via Ollama. A private experience integrated into your workflow, giving you full control.

## Features
- [ ] Option to include or exclude the current buffer content as context for model
- [ ] Enable switching between locally available models
- [x] Configuration of model options (e.g., temperature, system prompt, seed)
- [x] Enable/disable streaming of generated responses into the chat buffer
